{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/am3lia-low/battle-of-BERTS-FOMC/blob/main/FOMC_Step1_Data_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f648669",
      "metadata": {
        "id": "1f648669"
      },
      "source": [
        "# FOMC Hawkish-Dovish Classification — Step 1: Data Preparation & LLM Labeling\n",
        "## DSA4265 Take-Home Assignment\n",
        "\n",
        "This notebook covers:\n",
        "1. Loading the FOMC Hawkish-Dovish dataset (Shah et al., 2023 — ACL)\n",
        "2. Exploratory Data Analysis\n",
        "3. LLM labeling of ambiguous sentences using Claude\n",
        "4. Comparison of LLM labels vs human annotations\n",
        "5. Train/validation/test split preparation\n",
        "\n",
        "**Dataset:** \"Trillion Dollar Words\" — ~2,480 annotated FOMC sentences  \n",
        "**Source:** [gtfintechlab/fomc_communication](https://huggingface.co/datasets/gtfintechlab/fomc_communication)  \n",
        "**Labels:** 0 = Dovish, 1 = Hawkish, 2 = Neutral  \n",
        "**Citation:** Shah, Paturi & Chava (2023). \"Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis.\" ACL 2023.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69771ec7",
      "metadata": {
        "id": "69771ec7"
      },
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce0765c3",
      "metadata": {
        "id": "ce0765c3"
      },
      "outputs": [],
      "source": [
        "!pip install datasets pandas matplotlib seaborn anthropic scikit-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "912a93ab",
      "metadata": {
        "id": "912a93ab"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_PATH = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "print(f\"Drive path: {DRIVE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a254bf5",
      "metadata": {
        "id": "9a254bf5"
      },
      "source": [
        "## 2. Load the FOMC Dataset\n",
        "\n",
        "The dataset comes from the \"Trillion Dollar Words\" paper (Shah et al., 2023), published at ACL 2023.\n",
        "It contains ~2,480 annotated sentences from three types of FOMC communications:\n",
        "- **Meeting Minutes** — reports from the 8 annual FOMC meetings\n",
        "- **Press Conference Transcripts** — Fed chair Q&A with reporters\n",
        "- **Speeches** — public addresses by FOMC members\n",
        "\n",
        "Each sentence is labeled as:\n",
        "- **0 = Dovish** — indicates future monetary policy easing (lower rates, more money supply)\n",
        "- **1 = Hawkish** — indicates future monetary policy tightening (higher rates, less money supply)\n",
        "- **2 = Neutral** — factual/balanced, no clear policy direction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1484b6d2",
      "metadata": {
        "id": "1484b6d2"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load from HuggingFace\n",
        "dataset = load_dataset(\"gtfintechlab/fomc_communication\")\n",
        "print(f\"Dataset splits: {list(dataset.keys())}\")\n",
        "print(f\"Train size: {len(dataset['train'])}\")\n",
        "print(f\"Test size: {len(dataset['test'])}\")\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_train_raw = dataset['train'].to_pandas()\n",
        "df_test_raw = dataset['test'].to_pandas()\n",
        "\n",
        "# Combine for EDA (we'll re-split later with a validation set)\n",
        "df_all = pd.concat([df_train_raw, df_test_raw], ignore_index=True)\n",
        "\n",
        "# Label mapping\n",
        "LABEL_MAP = {0: 'Dovish', 1: 'Hawkish', 2: 'Neutral'}\n",
        "LABEL_MAP_REV = {v: k for k, v in LABEL_MAP.items()}\n",
        "df_all['label_name'] = df_all['label'].map(LABEL_MAP)\n",
        "\n",
        "print(f\"\\nTotal sentences: {len(df_all)}\")\n",
        "print(f\"\\nSample data:\")\n",
        "df_all[['sentence', 'label', 'label_name', 'year']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c06726f",
      "metadata": {
        "id": "1c06726f"
      },
      "source": [
        "## 3. Exploratory Data Analysis\n",
        "\n",
        "Key questions:\n",
        "1. How balanced are the three classes?\n",
        "2. How long are the sentences?\n",
        "3. How are sentences distributed across years?\n",
        "4. Are there differences in sentence length by class?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd9cefc4",
      "metadata": {
        "id": "cd9cefc4"
      },
      "outputs": [],
      "source": [
        "# 3.1 Class distribution\n",
        "print(\"CLASS DISTRIBUTION\")\n",
        "print(\"=\" * 50)\n",
        "class_counts = df_all['label_name'].value_counts()\n",
        "class_pcts = df_all['label_name'].value_counts(normalize=True) * 100\n",
        "for label in ['Dovish', 'Hawkish', 'Neutral']:\n",
        "    print(f\"  {label:<10} {class_counts[label]:>5}  ({class_pcts[label]:.1f}%)\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart\n",
        "colors = {'Dovish': '#3498db', 'Hawkish': '#e74c3c', 'Neutral': '#95a5a6'}\n",
        "bars = axes[0].bar(class_counts.index, class_counts.values,\n",
        "                   color=[colors[x] for x in class_counts.index])\n",
        "axes[0].set_title('FOMC Sentence Class Distribution')\n",
        "axes[0].set_ylabel('Count')\n",
        "for bar, count in zip(bars, class_counts.values):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 10,\n",
        "                 str(count), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%',\n",
        "            colors=[colors[x] for x in class_counts.index], startangle=90)\n",
        "axes[1].set_title('Class Proportions')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(DRIVE_PATH + 'class_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad1f4cf",
      "metadata": {
        "id": "dad1f4cf"
      },
      "outputs": [],
      "source": [
        "# 3.2 Sentence length analysis\n",
        "df_all['word_count'] = df_all['sentence'].str.split().str.len()\n",
        "df_all['char_count'] = df_all['sentence'].str.len()\n",
        "\n",
        "print(\"SENTENCE LENGTH STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Mean words:   {df_all['word_count'].mean():.1f}\")\n",
        "print(f\"  Median words: {df_all['word_count'].median():.1f}\")\n",
        "print(f\"  Min words:    {df_all['word_count'].min()}\")\n",
        "print(f\"  Max words:    {df_all['word_count'].max()}\")\n",
        "print(f\"  Std words:    {df_all['word_count'].std():.1f}\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Overall word count distribution\n",
        "axes[0].hist(df_all['word_count'], bins=50, color='#3498db', alpha=0.7, edgecolor='white')\n",
        "axes[0].axvline(df_all['word_count'].mean(), color='red', linestyle='--', label=f\"Mean: {df_all['word_count'].mean():.1f}\")\n",
        "axes[0].set_xlabel('Word Count')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Sentence Length Distribution')\n",
        "axes[0].legend()\n",
        "\n",
        "# Word count by class\n",
        "for label, color in colors.items():\n",
        "    subset = df_all[df_all['label_name'] == label]['word_count']\n",
        "    axes[1].hist(subset, bins=40, alpha=0.5, label=f\"{label} (mean: {subset.mean():.1f})\",\n",
        "                 color=color, edgecolor='white')\n",
        "axes[1].set_xlabel('Word Count')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Sentence Length by Class')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(DRIVE_PATH + 'sentence_lengths.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Per-class stats\n",
        "print(\"\\nWORD COUNT BY CLASS\")\n",
        "print(\"=\" * 50)\n",
        "for label in ['Dovish', 'Hawkish', 'Neutral']:\n",
        "    subset = df_all[df_all['label_name'] == label]['word_count']\n",
        "    print(f\"  {label:<10} Mean: {subset.mean():.1f}  Median: {subset.median():.1f}  Std: {subset.std():.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0755dbf5",
      "metadata": {
        "id": "0755dbf5"
      },
      "outputs": [],
      "source": [
        "# 3.3 Temporal distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Sentences per year\n",
        "year_counts = df_all['year'].value_counts().sort_index()\n",
        "axes[0].bar(year_counts.index, year_counts.values, color='#3498db', alpha=0.7)\n",
        "axes[0].set_xlabel('Year')\n",
        "axes[0].set_ylabel('Number of Sentences')\n",
        "axes[0].set_title('Sentences by Year')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Class distribution over time (stacked)\n",
        "year_class = df_all.groupby(['year', 'label_name']).size().unstack(fill_value=0)\n",
        "year_class_pct = year_class.div(year_class.sum(axis=1), axis=0) * 100\n",
        "year_class_pct[['Dovish', 'Hawkish', 'Neutral']].plot(\n",
        "    kind='bar', stacked=True, ax=axes[1],\n",
        "    color=[colors['Dovish'], colors['Hawkish'], colors['Neutral']])\n",
        "axes[1].set_xlabel('Year')\n",
        "axes[1].set_ylabel('Percentage')\n",
        "axes[1].set_title('Class Distribution Over Time')\n",
        "axes[1].legend(title='Stance', bbox_to_anchor=(1.05, 1))\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(DRIVE_PATH + 'temporal_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4bc4d7",
      "metadata": {
        "id": "3a4bc4d7"
      },
      "outputs": [],
      "source": [
        "# 3.4 Example sentences per class\n",
        "print(\"EXAMPLE SENTENCES BY CLASS\")\n",
        "print(\"=\" * 80)\n",
        "for label in ['Dovish', 'Hawkish', 'Neutral']:\n",
        "    print(f\"\\n--- {label.upper()} ---\")\n",
        "    samples = df_all[df_all['label_name'] == label].sample(3, random_state=42)\n",
        "    for _, row in samples.iterrows():\n",
        "        print(f\"  [{row['year']}] {row['sentence'][:150]}...\")\n",
        "        if len(row['sentence']) > 150:\n",
        "            print(f\"          ...{row['sentence'][150:]}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8411f705",
      "metadata": {
        "id": "8411f705"
      },
      "source": [
        "## 4. LLM Labeling with Claude\n",
        "\n",
        "We use Claude (Sonnet) to independently label FOMC sentences as hawkish, dovish, or neutral,\n",
        "then compare against the human annotations.\n",
        "\n",
        "**Approach:**\n",
        "- Send sentences in batches of 25 for efficiency\n",
        "- Ask Claude to classify from an investor's perspective (matching the original annotation guidelines)\n",
        "- Compare Claude's labels to human majority vote\n",
        "- Analyze where Claude disagrees with human annotators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc453ce",
      "metadata": {
        "id": "ebc453ce"
      },
      "outputs": [],
      "source": [
        "# Set up Anthropic client\n",
        "from anthropic import Anthropic\n",
        "from google.colab import userdata\n",
        "\n",
        "input_api_key = input(\"Enter your Claude API Key here: \")\n",
        "client = Anthropic(api_key=input_api_key)\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a monetary policy expert classifying sentences from Federal Reserve (FOMC) communications.\n",
        "\n",
        "For each sentence, classify it as:\n",
        "- HAWKISH: Indicates future monetary policy tightening (raising interest rates, reducing money supply, concerns about inflation being too high)\n",
        "- DOVISH: Indicates future monetary policy easing (lowering interest rates, increasing money supply, concerns about weak employment/growth)\n",
        "- NEUTRAL: Factual, balanced, or not clearly signaling either direction\n",
        "\n",
        "Consider the sentence from an investor's perspective: would this signal tighter policy (hawkish), looser policy (dovish), or neither (neutral)?\n",
        "\n",
        "IMPORTANT: Respond ONLY with the classification for each sentence in the exact format:\n",
        "idx: LABEL\n",
        "where idx is the sentence number and LABEL is one of HAWKISH, DOVISH, or NEUTRAL.\n",
        "No explanations, no extra text.\"\"\"\n",
        "\n",
        "def label_batch(sentences, start_idx=0):\n",
        "    \"\"\"Label a batch of sentences using Claude.\"\"\"\n",
        "    # Format sentences with indices\n",
        "    formatted = \"\\n\".join([f\"{start_idx + i}: {s}\" for i, s in enumerate(sentences)])\n",
        "    prompt = f\"Classify each sentence:\\n\\n{formatted}\"\n",
        "\n",
        "    response = client.messages.create(\n",
        "        model=\"claude-sonnet-4-20250514\",\n",
        "        max_tokens=1000,\n",
        "        system=SYSTEM_PROMPT,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    # Parse response\n",
        "    results = {}\n",
        "    for line in response.content[0].text.strip().split(\"\\n\"):\n",
        "        line = line.strip()\n",
        "        if \":\" in line:\n",
        "            parts = line.split(\":\", 1)\n",
        "            try:\n",
        "                idx = int(parts[0].strip())\n",
        "                label = parts[1].strip().upper()\n",
        "                if label in ['HAWKISH', 'DOVISH', 'NEUTRAL']:\n",
        "                    results[idx] = label\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "    return results\n",
        "\n",
        "# Test with a small batch\n",
        "test_sentences = [\n",
        "    \"And that, I think, does raise the risk that high inflation will be more persistent.\",\n",
        "    \"The committee will continue to monitor incoming data.\",\n",
        "    \"Recent declines in payroll employment, while still sizable, were smaller than earlier.\"\n",
        "]\n",
        "test_results = label_batch(test_sentences)\n",
        "print(\"Test results:\")\n",
        "for idx, label in test_results.items():\n",
        "    print(f\"  {idx}: {label} -> '{test_sentences[idx][:80]}...'\")\n",
        "print(\"\\nLLM labeling pipeline works!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a556fb7",
      "metadata": {
        "id": "8a556fb7"
      },
      "outputs": [],
      "source": [
        "# Label ALL sentences in batches\n",
        "BATCH_SIZE = 25\n",
        "all_sentences = df_all['sentence'].tolist()\n",
        "llm_labels = {}\n",
        "\n",
        "print(f\"Labeling {len(all_sentences)} sentences in batches of {BATCH_SIZE}...\")\n",
        "print(f\"Estimated batches: {len(all_sentences) // BATCH_SIZE + 1}\")\n",
        "print()\n",
        "\n",
        "for batch_start in range(0, len(all_sentences), BATCH_SIZE):\n",
        "    batch_end = min(batch_start + BATCH_SIZE, len(all_sentences))\n",
        "    batch = all_sentences[batch_start:batch_end]\n",
        "\n",
        "    batch_num = batch_start // BATCH_SIZE + 1\n",
        "    total_batches = len(all_sentences) // BATCH_SIZE + 1\n",
        "\n",
        "    try:\n",
        "        results = label_batch(batch, start_idx=batch_start)\n",
        "        llm_labels.update(results)\n",
        "\n",
        "        if batch_num % 10 == 0 or batch_num == 1:\n",
        "            print(f\"  Batch {batch_num}/{total_batches} done — {len(llm_labels)} labels so far\")\n",
        "\n",
        "        time.sleep(1)  # Rate limiting\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error in batch {batch_num}: {e}\")\n",
        "        time.sleep(5)\n",
        "        # Retry once\n",
        "        try:\n",
        "            results = label_batch(batch, start_idx=batch_start)\n",
        "            llm_labels.update(results)\n",
        "        except Exception as e2:\n",
        "            print(f\"  Retry failed: {e2}\")\n",
        "            continue\n",
        "\n",
        "print(f\"\\nDone! Labeled {len(llm_labels)} / {len(all_sentences)} sentences ({len(llm_labels)/len(all_sentences)*100:.1f}%)\")\n",
        "\n",
        "# Save raw LLM labels\n",
        "with open(DRIVE_PATH + 'llm_labels_fomc.json', 'w') as f:\n",
        "    json.dump(llm_labels, f, indent=2)\n",
        "print(f\"Saved to: {DRIVE_PATH}llm_labels_fomc.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b34430",
      "metadata": {
        "id": "02b34430"
      },
      "source": [
        "## 5. LLM vs Human Label Comparison\n",
        "\n",
        "How well does Claude agree with the human annotators? Where does it disagree, and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f80c5061",
      "metadata": {
        "id": "f80c5061"
      },
      "outputs": [],
      "source": [
        "# Map LLM labels to numeric\n",
        "LLM_LABEL_MAP = {'DOVISH': 0, 'HAWKISH': 1, 'NEUTRAL': 2}\n",
        "\n",
        "# Add LLM labels to dataframe\n",
        "df_all['llm_label_str'] = df_all.index.map(lambda i: llm_labels.get(i, None))\n",
        "df_all['llm_label'] = df_all['llm_label_str'].map(LLM_LABEL_MAP)\n",
        "\n",
        "# Filter to sentences where we have both labels\n",
        "df_compare = df_all.dropna(subset=['llm_label']).copy()\n",
        "df_compare['llm_label'] = df_compare['llm_label'].astype(int)\n",
        "\n",
        "print(f\"Sentences with both human and LLM labels: {len(df_compare)}\")\n",
        "print(f\"Missing LLM labels: {len(df_all) - len(df_compare)}\")\n",
        "\n",
        "# Overall agreement\n",
        "agreement = (df_compare['label'] == df_compare['llm_label']).mean()\n",
        "print(f\"\\nOverall agreement: {agreement:.1%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dff49163",
      "metadata": {
        "id": "dff49163"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n",
        "\n",
        "# Confusion matrix: Human (rows) vs LLM (columns)\n",
        "labels = [0, 1, 2]\n",
        "label_names = ['Dovish', 'Hawkish', 'Neutral']\n",
        "\n",
        "cm = confusion_matrix(df_compare['label'], df_compare['llm_label'], labels=labels)\n",
        "cm_norm = confusion_matrix(df_compare['label'], df_compare['llm_label'], labels=labels, normalize='true')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=label_names, yticklabels=label_names)\n",
        "axes[0].set_xlabel('LLM Label')\n",
        "axes[0].set_ylabel('Human Label')\n",
        "axes[0].set_title('Human vs LLM Labels (Counts)')\n",
        "\n",
        "sns.heatmap(cm_norm, annot=True, fmt='.1%', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=label_names, yticklabels=label_names)\n",
        "axes[1].set_xlabel('LLM Label')\n",
        "axes[1].set_ylabel('Human Label')\n",
        "axes[1].set_title('Human vs LLM Labels (Normalized)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(DRIVE_PATH + 'llm_vs_human_confusion.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Classification report (treating human labels as ground truth)\n",
        "print(\"LLM PERFORMANCE vs HUMAN ANNOTATIONS\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(df_compare['label'], df_compare['llm_label'],\n",
        "                           target_names=label_names, digits=3))\n",
        "\n",
        "kappa = cohen_kappa_score(df_compare['label'], df_compare['llm_label'])\n",
        "print(f\"Cohen's Kappa: {kappa:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43298a7b",
      "metadata": {
        "id": "43298a7b"
      },
      "outputs": [],
      "source": [
        "# Per-class agreement\n",
        "print(\"PER-CLASS AGREEMENT RATES\")\n",
        "print(\"=\" * 50)\n",
        "for label_id, label_name in LABEL_MAP.items():\n",
        "    mask = df_compare['label'] == label_id\n",
        "    if mask.sum() > 0:\n",
        "        class_agreement = (df_compare.loc[mask, 'label'] == df_compare.loc[mask, 'llm_label']).mean()\n",
        "        print(f\"  {label_name:<10} {class_agreement:.1%}  (n={mask.sum()})\")\n",
        "\n",
        "# Interesting disagreements\n",
        "print(\"\\nINTERESTING DISAGREEMENTS (Human vs LLM)\")\n",
        "print(\"=\" * 80)\n",
        "disagreements = df_compare[df_compare['label'] != df_compare['llm_label']].copy()\n",
        "disagreements['human_name'] = disagreements['label'].map(LABEL_MAP)\n",
        "disagreements['llm_name'] = disagreements['llm_label'].map(LABEL_MAP)\n",
        "\n",
        "# Sample disagreements by type\n",
        "for (h, l), group in disagreements.groupby(['human_name', 'llm_name']):\n",
        "    if len(group) >= 2:\n",
        "        print(f\"\\n  Human={h}, LLM={l} ({len(group)} cases):\")\n",
        "        for _, row in group.head(2).iterrows():\n",
        "            sent = row['sentence'][:120]\n",
        "            print(f\"    [{row['year']}] {sent}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8eb8338",
      "metadata": {
        "id": "b8eb8338"
      },
      "source": [
        "## 6. Prepare Train / Validation / Test Splits\n",
        "\n",
        "The original dataset provides a train/test split. We further split the training data\n",
        "to create a validation set: **70% train / 15% validation / 15% test** (stratified by label)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f05505",
      "metadata": {
        "id": "69f05505"
      },
      "outputs": [],
      "source": [
        "# We'll use the original test set and split the original train into train + val\n",
        "# This preserves the original authors' test set for fair comparison\n",
        "\n",
        "df_train_orig = df_all.iloc[:len(df_train_raw)].copy()\n",
        "df_test = df_all.iloc[len(df_train_raw):].copy()\n",
        "\n",
        "# Split original train into new train + validation (85/15 of original train ≈ 70/15 overall)\n",
        "df_train, df_val = train_test_split(\n",
        "    df_train_orig,\n",
        "    test_size=0.15,\n",
        "    stratify=df_train_orig['label'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"DATASET SPLITS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Train:      {len(df_train):>6}  ({len(df_train)/len(df_all)*100:.1f}%)\")\n",
        "print(f\"  Validation: {len(df_val):>6}  ({len(df_val)/len(df_all)*100:.1f}%)\")\n",
        "print(f\"  Test:       {len(df_test):>6}  ({len(df_test)/len(df_all)*100:.1f}%)\")\n",
        "print(f\"  Total:      {len(df_all):>6}\")\n",
        "\n",
        "# Verify stratification\n",
        "print(\"\\nLABEL DISTRIBUTION PER SPLIT\")\n",
        "print(\"=\" * 50)\n",
        "for split_name, split_df in [('Train', df_train), ('Val', df_val), ('Test', df_test)]:\n",
        "    dist = split_df['label_name'].value_counts(normalize=True) * 100\n",
        "    print(f\"  {split_name:<12} Dovish: {dist.get('Dovish', 0):.1f}%  Hawkish: {dist.get('Hawkish', 0):.1f}%  Neutral: {dist.get('Neutral', 0):.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eba09869",
      "metadata": {
        "id": "eba09869"
      },
      "outputs": [],
      "source": [
        "# Save splits\n",
        "df_train.to_csv(DRIVE_PATH + 'train.csv', index=False)\n",
        "df_val.to_csv(DRIVE_PATH + 'val.csv', index=False)\n",
        "df_test.to_csv(DRIVE_PATH + 'test.csv', index=False)\n",
        "\n",
        "print(f\"Saved train.csv ({len(df_train)} rows)\")\n",
        "print(f\"Saved val.csv ({len(df_val)} rows)\")\n",
        "print(f\"Saved test.csv ({len(df_test)} rows)\")\n",
        "\n",
        "# Also save the full dataset with LLM labels for later analysis\n",
        "df_all.to_csv(DRIVE_PATH + 'fomc_all_with_llm.csv', index=False)\n",
        "print(f\"Saved fomc_all_with_llm.csv ({len(df_all)} rows)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fae6593c",
      "metadata": {
        "id": "fae6593c"
      },
      "source": [
        "## 7. Summary\n",
        "\n",
        "### Dataset Overview\n",
        "- **Source:** \"Trillion Dollar Words\" (Shah et al., ACL 2023)\n",
        "- **Size:** ~2,480 annotated FOMC sentences (1996–2022)\n",
        "- **Classes:** Dovish / Hawkish / Neutral (3-class)\n",
        "- **Source types:** Meeting minutes, press conferences, speeches\n",
        "\n",
        "### Key EDA Findings\n",
        "- Class distribution is imbalanced (neutral-heavy, typical for FOMC text)\n",
        "- Sentence lengths vary considerably — some are very long multi-clause constructions\n",
        "- Temporal coverage spans multiple monetary policy regimes (dot-com, GFC, COVID, inflation era)\n",
        "\n",
        "### LLM Labeling\n",
        "- Claude independently labeled all sentences\n",
        "- Agreement rate and Cohen's Kappa reported above\n",
        "- Disagreements reveal genuinely ambiguous monetary policy language\n",
        "\n",
        "### Files Saved\n",
        "- `train.csv`, `val.csv`, `test.csv` — stratified splits\n",
        "- `fomc_all_with_llm.csv` — full dataset with LLM labels\n",
        "- `llm_labels_fomc.json` — raw LLM label outputs\n",
        "- EDA charts: `class_distribution.png`, `sentence_lengths.png`, `temporal_distribution.png`\n",
        "- Comparison chart: `llm_vs_human_confusion.png`\n",
        "\n",
        "### Next Step\n",
        "- **Step 2:** Train FinBERT and RoBERTa models, evaluate and compare"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}